name: Performance Test

on:
  workflow_dispatch:
    inputs:
      file_size_kb:
        description: 'File size in KB'
        required: false
        default: '1024'
      max_concurrency:
        description: 'Maximum concurrent writers'
        required: false
        default: '16'
      duration_seconds:
        description: 'Duration per concurrency level (seconds)'
        required: false
        default: '30'
  schedule:
    - cron: '0 4 * * 0'  # Weekly on Sunday at 4am UTC

jobs:
  performance:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Go
        uses: actions/setup-go@v5

      - name: Build HelmetFS
        run: go build -o helmetfs .

      - name: Start HelmetFS
        run: |
          ./helmetfs -addr :8080 &
          sleep 2

      - name: Run throughput benchmark
        run: |
          cd perftest
          go run . \
            -url http://localhost:8080 \
            -file-size $(( ${{ github.event.inputs.file_size_kb || '1024' }} * 1024 )) \
            -max-concurrency ${{ github.event.inputs.max_concurrency || '16' }} \
            -duration ${{ github.event.inputs.duration_seconds || '30' }}s \
            -warmup 5s \
            -output results.json

      - name: Generate visualization
        run: |
          cat > generate_chart.py << 'PYTHON'
          import json
          import sys

          with open('perftest/results.json') as f:
              data = json.load(f)

          results = data['results']
          file_size_kb = data['file_size_kb']

          # Generate ASCII chart for throughput
          print(f"\n## Throughput Performance ({file_size_kb} KB files)\n")
          print("```")
          print("Throughput (MB/s) vs Concurrent Writers")
          print("=" * 60)

          max_throughput = max(r['throughput_mbps'] for r in results)
          bar_width = 40

          for r in results:
              bar_len = int(r['throughput_mbps'] / max_throughput * bar_width) if max_throughput > 0 else 0
              bar = '█' * bar_len + '░' * (bar_width - bar_len)
              print(f"{r['concurrency']:>2} writers │ {bar} │ {r['throughput_mbps']:>7.2f} MB/s")

          print("=" * 60)
          print("```\n")

          # Generate latency table
          print("### Latency Statistics\n")
          print("| Concurrency | Avg (ms) | P50 (ms) | P95 (ms) | P99 (ms) | Errors |")
          print("|-------------|----------|----------|----------|----------|--------|")
          for r in results:
              print(f"| {r['concurrency']:>11} | {r['avg_latency_ms']:>8.1f} | {r['p50_latency_ms']:>8.1f} | {r['p95_latency_ms']:>8.1f} | {r['p99_latency_ms']:>8.1f} | {r['errors']:>6} |")
          print("\n")

          # Generate ops/sec chart
          print("### Operations per Second\n")
          print("```")
          print("Ops/sec vs Concurrent Writers")
          print("-" * 60)

          max_ops = max(r['ops_per_sec'] for r in results)

          for r in results:
              bar_len = int(r['ops_per_sec'] / max_ops * bar_width) if max_ops > 0 else 0
              bar = '▓' * bar_len + '░' * (bar_width - bar_len)
              print(f"{r['concurrency']:>2} writers │ {bar} │ {r['ops_per_sec']:>7.1f} ops/s")

          print("-" * 60)
          print("```\n")

          # Generate Mermaid chart data (for GitHub rendering)
          print("<details>")
          print("<summary>Throughput Chart (Mermaid)</summary>\n")
          print("```mermaid")
          print("xychart-beta")
          print(f'    title "Write Throughput ({file_size_kb} KB files)"')
          print('    x-axis "Concurrent Writers" [' + ', '.join(str(r['concurrency']) for r in results) + ']')
          print('    y-axis "Throughput (MB/s)"')
          print('    bar [' + ', '.join(f"{r['throughput_mbps']:.2f}" for r in results) + ']')
          print("```")
          print("</details>\n")

          print("<details>")
          print("<summary>Latency Chart (Mermaid)</summary>\n")
          print("```mermaid")
          print("xychart-beta")
          print('    title "Write Latency by Concurrency"')
          print('    x-axis "Concurrent Writers" [' + ', '.join(str(r['concurrency']) for r in results) + ']')
          print('    y-axis "Latency (ms)"')
          print('    line "P50" [' + ', '.join(f"{r['p50_latency_ms']:.1f}" for r in results) + ']')
          print('    line "P95" [' + ', '.join(f"{r['p95_latency_ms']:.1f}" for r in results) + ']')
          print('    line "P99" [' + ', '.join(f"{r['p99_latency_ms']:.1f}" for r in results) + ']')
          print("```")
          print("</details>\n")

          # Summary
          peak_throughput = max(r['throughput_mbps'] for r in results)
          peak_concurrency = next(r['concurrency'] for r in results if r['throughput_mbps'] == peak_throughput)
          total_errors = sum(r['errors'] for r in results)

          print("### Summary\n")
          print(f"- **Peak Throughput:** {peak_throughput:.2f} MB/s at {peak_concurrency} concurrent writers")
          print(f"- **File Size:** {file_size_kb} KB")
          print(f"- **Total Errors:** {total_errors}")
          PYTHON

          python3 generate_chart.py | tee performance_report.md

      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: performance-results
          path: |
            perftest/results.json
            performance_report.md

      - name: Post results to job summary
        run: cat performance_report.md >> $GITHUB_STEP_SUMMARY
